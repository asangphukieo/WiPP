#!/usr/bin/env python3

""" Snakefile for the second Pipeline part: Training of a classifier, 
parameter optimization, actuall peak picking, merging to a high confidence peak
set, and cross-sample alignment.

"""

import sys
import os
# Add path to R and python libraries
base_dir = '..'
py_lib_dir = os.path.join(base_dir, 'lib', 'python')
R_lib_dir = os.path.join(base_dir, 'lib', 'R')
R_utils_file = os.path.join(R_lib_dir, 'utils.R')
sys.path.append(py_lib_dir)
# Import Pipeline modules
import utils
import peak_picking as pp
import peak_alignment as pa

# Load config file
configfile: 'config.yaml'

# proof check config data and get relevant data
algorithms, sample_data, opt_data, wash_file = \
    utils.evaluate_peakpicking_config(config)

groupCorr_str = '' if not config['XCMS_params']['groupCorr'] else '--groupCorr'


rule all:
    input:
        expand(
            '04_Final_results/{group}__final.csv',
            group=sample_data['groups'] + ['all']
        )


# ------------------------------------------------------------------------------
# -------------------------------- CLASSIFIER ----------------------------------
# ------------------------------------------------------------------------------


rule train_classifier:
    output:
        '01_Classifier/{algorithm}__classifier.pkl'
    params:
        tr_dir = os.path.join(
            config['static_data']['data_path'],
            config['training_data-general']['training_samples']
        ),
        f_type = sample_data['type'],
        val_set_ratio = config['classifier']['validation_set_size'],
        cw_split = config['classifier']['cross_validation_splits'],
        SVM = config['classifier']['SVM'],
        RF = config['classifier']['RF']
    threads:
        config['static_data']['cores']
    run:
        old_clf_path = config['classifier']['path']
        if old_clf_path:
            pp.copy_classifier(old_clf_path, wildcards['algorithm'])
        else:
            pp.generate_classifier(
                wildcards.algorithm,
                params.tr_dir,
                params.f_type,
                validation_size=params.val_set_ratio,
                cv=params.cw_split,
                SVM=params.SVM,
                RF=params.RF,
                cores=threads
            )


# ------------------------------------------------------------------------------
# --------------------------------- CENTWAVE -----------------------------------
# ------------------------------------------------------------------------------

rule run_grid_search_XCMS_CW:
    input:
        os.path.join(
            config['static_data']['data_path'],
            config['optimization-general']['optimization_samples'],
            '{}.{}'.format('{XCMS_opt_file}', sample_data['type'])
        )
    output:
        '02_Optimization/Grid_search/XCMS-CW/{XCMS_opt_file}__{parameter_CW}.csv'
    params:
        cores = 2,
        groupCorr = groupCorr_str,
        R_dir = R_lib_dir,
        utils = R_utils_file,
        pwMin = lambda wildcards: \
            re.search('par0-(\d+\.\d+|\d+)', wildcards.parameter_CW).group(1),
        pwMax = lambda wildcards: \
            re.search('par1-(\d+\.\d+|\d+)', wildcards.parameter_CW).group(1),
        mzdiff = lambda wildcards: '--mzdiff {}'.format(
                    re.search('par2-{1}(-?\d+\.\d+|\d+)', wildcards.parameter_CW)\
                    .group(1).replace('-', 'm')
                ) if config['static_data'].get('high_resolution', False) else '',
        ppm = lambda wildcards: '--ppm {}'.format(
                re.search('par3-(\d+\.\d+|\d+)', wildcards.parameter_CW) \
                    .group(1)
            ) if config['static_data'].get('high_resolution', False) else ''
    shell:
        'Rscript {params.R_dir}/run_XCMS-CW.R {input} {output} '
        '--peakwidth {params.pwMin} {params.pwMax} '
        '{params.mzdiff} {params.ppm} '
        '--cores {params.cores} {params.groupCorr} --utils {params.utils}'
        


rule classify_grid_search_XCMS_CW:
    input:
        clf = '01_Classifier/XCMS-CW__classifier.pkl',
        file = '02_Optimization/Grid_search/XCMS-CW/' \
            '{XCMS_opt_file}__{parameter_CW}.csv',
    output:
        '02_Optimization/Classified/XCMS-CW/' \
            '{XCMS_opt_file}__{parameter_CW}__classified.csv'
    params:
        raw_data = config['static_data']['data_path'],
        opt_dir = config['optimization-general']['optimization_samples'],
        f_type = sample_data['type']
    run:
        sample_file = os.path.join(
            params.raw_data, params.opt_dir,
            '{}.{}'.format(wildcards.XCMS_opt_file, params.f_type)
        )
        pp.classify_XCMS_peaks(input.clf, input.file, output[0], sample_file)



rule evaluate_grid_search_XCMS_CW:
    input:
        expand(
            '02_Optimization/Classified/XCMS-CW/' \
                '{XCMS_opt_file}__{parameter_CW}__classified.csv',
            XCMS_opt_file=opt_data['files'],
            parameter_CW=opt_data['params']['XCMS-CW'],
        )
    output:
        '02_Optimization/00__XCMS-CW__best_parameters.csv'
    wildcard_constraints:
        XCMS_opt_file = '.*(?!__)',
        parameter_CW = '.*(?=__)'
    params:
        raw_data = config['static_data']['data_path'],
        f_type = sample_data['type']
    run:
        pp.evaluate_grid_search(
            input, output[0], 
            params.raw_data, params.f_type, config['optimization-score']
        )


rule run_XCMS_CW_all:
    input:
        '02_Optimization/00__XCMS-CW__best_parameters.csv',
    output:
        '03_Peak_Picking/XCMS-CW/{XCMS_all_file}__peaks.csv',
    conda:
        os.path.join('..', 'envs', 'R_env.yaml')
    params:
        cores = 2,
        groupCorr = groupCorr_str,
        R_dir = R_lib_dir,
        utils = R_utils_file,
        raw_data_file = lambda wildcards: os.path.join(
            config['static_data']['data_path'],
            '{}.{}'.format(wildcards.XCMS_all_file, sample_data['type'])
        ),
        **utils.read_params_CW(
            '02_Optimization/00__XCMS-CW__best_parameters.csv'
        )
    shell:
        'Rscript {params.R_dir}/run_XCMS-CW.R {params.raw_data_file} {output} '
        '--peakwidth {params.pwMin} {params.pwMax} {params.mzdiff} {params.ppm} '
        '--cores {params.cores} {params.groupCorr} --utils {params.utils}'
        

rule classify_XCMS_CW:
    input:
        file = '03_Peak_Picking/XCMS-CW/{XCMS_all_file}__peaks.csv',
        clf = '01_Classifier/XCMS-CW__classifier.pkl'
    output:
        '04_Final_results/00_Classified/XCMS-CW/{XCMS_all_file}__classified.csv'
    params:
        raw_data = config['static_data']['data_path'],
        f_type = sample_data['type']
    run:
        sample_file = os.path.join(
            params.raw_data,
            '{}.{}'.format(wildcards.XCMS_all_file, params.f_type)
        )
        pp.classify_XCMS_peaks(input.clf, input.file, output[0], sample_file)


# ------------------------------------------------------------------------------
# ------------------------------- MATCHED FILTER -------------------------------
# ------------------------------------------------------------------------------
      
rule run_grid_search_XCMS_MF:
    input:
        os.path.join(
            config['static_data']['data_path'],
            config['optimization-general']['optimization_samples'],
            '{}.{}'.format('{XCMS_opt_file}', sample_data['type'])
        )
    output:
        '02_Optimization/Grid_search/XCMS-MF/{XCMS_opt_file}__{parameter_MF}.csv'
    params:
        cores = 2,
        groupCorr = groupCorr_str,
        R_dir = R_lib_dir,
        utils = R_utils_file,
        fwhm = lambda wildcards: \
            re.search('par0-(\d+\.\d+|\d+)', wildcards.parameter_MF).group(1),
        sn = lambda wildcards: \
            re.search('par1-(\d+\.\d+|\d+)', wildcards.parameter_MF).group(1),
        mzdiff = lambda wildcards: '--mzdiff {}'.format(
                re.search('par2-{1}(-?\d+\.\d+|\d+)', wildcards.parameter_MF) \
                    .group(1).replace('-', 'm')
            ) if config['static_data'].get('high_resolution', False) else '',
        step = lambda wildcards: '--step {}'.format(
                re.search('par3-(\d+\.\d+|\d+)', wildcards.parameter_MF) \
                    .group(1)
            )  if config['static_data'].get('high_resolution', False) else '',
        steps = lambda wildcards: '--steps {}'.format(
                re.search('par4-(\d+\.\d+|\d+)', wildcards.parameter_MF) \
                    .group(1)
            ) if config['static_data'].get('high_resolution', False) else ''
    shell:
        'Rscript {params.R_dir}/run_XCMS-MF.R {input} {output} '
        '--fwhm {params.fwhm} --sn {params.sn} '
        '{params.mzdiff} {params.step} {params.steps} '
        '--cores {params.cores} {params.groupCorr} --utils {params.utils}'
        


rule classify_grid_search_XCMS_MF:
    input:
        clf = '01_Classifier/XCMS-MF__classifier.pkl',
        file = '02_Optimization/Grid_search/XCMS-MF/' \
            '{XCMS_opt_file}__{parameter_MF}.csv',
    output:
        '02_Optimization/Classified/XCMS-MF/' \
            '{XCMS_opt_file}__{parameter_MF}__classified.csv'
    params:
        raw_data = config['static_data']['data_path'],
        opt_dir = config['optimization-general']['optimization_samples'],
        f_type = sample_data['type']
    run:
        sample_file = os.path.join(
            params.raw_data, params.opt_dir,
            '{}.{}'.format(wildcards.XCMS_opt_file, params.f_type)
        )
        pp.classify_XCMS_peaks(input.clf, input.file, output[0], sample_file)


rule evaluate_grid_search_XCMS_MF:
    input:
        expand(
            '02_Optimization/Classified/XCMS-MF/' \
                '{XCMS_opt_file}__{parameter_MF}__classified.csv',
            XCMS_opt_file=opt_data['files'],
            parameter_MF=opt_data['params']['XCMS-MF'],
        )
    output:
        '02_Optimization/00__XCMS-MF__best_parameters.csv'
    wildcard_constraints:
        XCMS_opt_file = '.*(!?__par)',
        parameter_MF = 'par.*(!?__)'
    params:
        raw_data = config['static_data']['data_path'],
        f_type = sample_data['type']
    run:
        pp.evaluate_grid_search(
            input, output[0], 
            params.raw_data, params.f_type, config['optimization-score']
        )


rule run_XCMS_MF_all:
    input:
        '02_Optimization/00__XCMS-MF__best_parameters.csv'
    output:
        '03_Peak_Picking/XCMS-MF/{XCMS_all_file}__peaks.csv',
    conda:
        os.path.join('..', 'envs', 'R_env.yaml')
    params:
        cores = 2,
        groupCorr = groupCorr_str,
        R_dir = R_lib_dir,
        utils = R_utils_file,
        raw_data_file = lambda wildcards: os.path.join(
            config['static_data']['data_path'],
            '{}.{}'.format(wildcards.XCMS_all_file, sample_data['type'])
        ),
        **utils.read_params_MF(
            '02_Optimization/00__XCMS-MF__best_parameters.csv'
        )
    shell:
        'Rscript {params.R_dir}/run_XCMS-MF.R {params.raw_data_file} {output} '
        '--fwhm {params.fwhm} --sn {params.sn} {params.mzdiff} {params.step} '
        '{params.steps} --cores {params.cores} {params.groupCorr} '
        '--utils {params.utils}'
        

rule classify_XCMS_MF:
    input:
        clf = '01_Classifier/XCMS-MF__classifier.pkl',
        file = '03_Peak_Picking/XCMS-MF/{XCMS_all_file}__peaks.csv',
    output:
        '04_Final_results/00_Classified/XCMS-MF/{XCMS_all_file}__classified.csv'
    params:
        raw_data = config['static_data']['data_path'],
        f_type = sample_data['type']
    run:
        sample_file = os.path.join(
            params.raw_data,
            '{}.{}'.format(wildcards.XCMS_all_file, params.f_type)
        )
        pp.classify_XCMS_peaks(input.clf, input.file, output[0], sample_file)


# ------------------------------------------------------------------------------
# ------------------------------- ALL ALGORITHMS -------------------------------
# ------------------------------------------------------------------------------

rule merge_all:
    input:
        expand(
            '04_Final_results/00_Classified/{algorithm}/' \
                '{{XCMS_all_file}}__classified.csv',
            algorithm=algorithms
        )
    output:
        '04_Final_results/01_Merged/{XCMS_all_file}__merged.csv'
    params:
        raw_data = config['static_data']['data_path'],
        f_type = sample_data['type'],
        peak_min_width = config['static_data']['peak_min_width'],
        peak_min_mz = config['static_data']['peak_min_mz'],
        tolerance = config['merging']['RT_tol']
    run:
        pp.merge_detected_peaks(
            input, output[0], params.raw_data, params.f_type,
            params.tolerance, params.peak_min_width, params.peak_min_mz
        )


if config['retention_index']['calculate']:
    align_tol = config['alignment']['RI_tol']
    align_files = '04_Final_results/02_RetentionIndex/' \
        '{XCMS_all_file}__merged_RI.csv'
    rule calc_retention_index:
        input:
            sample = '04_Final_results/01_Merged/{XCMS_all_file}__merged.csv',
            wash = '04_Final_results/01_Merged/{}__merged.csv'.format(wash_file)
        output:
            '04_Final_results/02_RetentionIndex/{XCMS_all_file}__merged_RI.csv'
        params:
            raw_data = config['static_data']['data_path'],
            f_type = sample_data['type'],
            alkanes = config['retention_index']['alkanes']
        run:
            pa.calc_RI_with_wash(
                input.sample, output[0], input.wash, params.alkanes
            )
else:
    align_tol = config['alignment']['RT_tol']
    align_files = '04_Final_results/01_Merged/{XCMS_all_file}__merged.csv'


rule merging_overview:
    input:
        expand(
            align_files,
            XCMS_all_file=sample_data['files']
        )
    output:
        '04_Final_results/01_Merged/overview.csv'
    run:
        with open(output[0], 'w') as f:
            f.write('\n'.join(sorted(input)))


rule align_across_groups:
    input:
        '04_Final_results/01_Merged/overview.csv'
    output:
        '04_Final_results/{group}__final.csv'
    params:
        tolerance = align_tol,
        min_samples = config['alignment']['min_samples'],
        min_sim = config['alignment']['min_similarity'],
        RI = config['retention_index']['calculate']
    run:
        with open(input[0], 'r') as f:
            merged_files = f.read().split('\n')
        grp_files = [
            i for i in merged_files \
                if wildcards.group in utils.split_path(i) \
                    or wildcards.group == 'all'
        ]
        pa.align_across_samples(
            grp_files, output[0],
            params.tolerance, params.min_samples, params.min_sim, params.RI
        )